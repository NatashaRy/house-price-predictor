{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# **04 - Feature Engineering**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## Objectives\n",
        "* Engineer features to improve the predictive power of the dataset and prepare it for building a regression model to predict house prices.\n",
        "* Handle missing values, outliers, and skewness to ensure data integrity and consistency.\n",
        "* Encode categorical variables and transform numerical variables to optimize them for regression modeling.\n",
        "* Identify and remove multicollinear features to enhance model interpretability and performance.\n",
        "* Ensure all variables are scaled and ready for regression modeling.\n",
        "\n",
        "## Inputs\n",
        "* Train set: `outputs/datasets/cleaned/train_set.parquet`\n",
        "* Test set: `outputs/datasets/cleaned/test_set.parquet`\n",
        "* Both datasets are used to ensure consistent feature engineering across training and testing phases.\n",
        "\n",
        "## Outputs\n",
        "* A list of final features selected for the regression model.\n",
        "* Train and test sets with engineered features.\n",
        "* Documentation of the transformations applied to variables and their impact on the dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWZXH9LwoQg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Change Working Directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ensure the working directory is set to the project root for consistent file paths. This ensures that all file paths work correctly, regardless of where the notebook is executed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.chdir(os.path.dirname(current_dir))\n",
        "print(\"You set a new current directory\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mavJ8DibrcQ"
      },
      "source": [
        "## 1. Load Clean Data and Data Profiling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We load the cleaned datasets (`train_set.parquet` and `test_set.parquet`) to ensure they are ready for feature engineering. This step is crucial to verify that the data is clean, consistent, and free from issues such as missing values, which could negatively impact the modeling process.\n",
        "\n",
        "### Purpose:\n",
        "* Ensure the datasets are properly loaded and accessible for feature engineering.\n",
        "* Identify and handle any remaining missing values to maintain data integrity.\n",
        "* Confirm the structure and size of the datasets to validate their readiness for further processing.\n",
        "\n",
        "### Datasets:\n",
        "* Train Set: `train_set.parquet` - Used for training the regression model.\n",
        "* Test Set: `test_set.parquet` - Used for evaluating the model's performance on unseen data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 1.1.1 Train Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import Pandas for data manipulation\n",
        "import pandas as pd\n",
        "\n",
        "# Create DataFrame of cleaned training dataset and display the first five rows\n",
        "TrainSet = pd.read_parquet(\"outputs/datasets/cleaned/train_set.parquet\")\n",
        "TrainSet.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 1.1.2 Test Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the cleaned test dataset and display the first five rows\n",
        "TestSet = pd.read_parquet(\"outputs/datasets/cleaned/test_set.parquet\")\n",
        "TestSet.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.2 Check Datasets for Missing Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check Train Set for missing values and display results\n",
        "print(f\"TrainSet has: {TrainSet.isnull().sum()} missing values.\")\n",
        "\n",
        "# Check Test Set for missing values and display results\n",
        "print(f\"TestSet has: {TestSet.isnull().sum()} missing values.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.3 Data Profiling for Exploratory Data Analysis (EDA)\n",
        "\n",
        "*Note: We are only processing `TrainSet` at this stage.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import Ydata Profiling\n",
        "from ydata_profiling import ProfileReport\n",
        "\n",
        "# Generate a profile report for the training set\n",
        "profile = ProfileReport(df=TrainSet, minimal=True)\n",
        "\n",
        "# Display the profile report in the notebook\n",
        "profile.to_notebook_iframe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY3l0-AxO93d"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFQo3ycuO-v6"
      },
      "source": [
        "## 2. Feature Engineering\n",
        "\n",
        "*Note: This code was inspired by feature engineering in Walkthrough Project 2: Churnometer.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1 Create Custom Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Feature engineering is a critical step to enhance the predictive power of the dataset. We use a modular approach to systematically handle outliers, apply transformations to improve variable distributions, and encode categorical variables. This ensures the dataset is optimized for regression modeling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import libraries for visualization\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Import statistical functions from the SciPy library for advanced statistical analysis\n",
        "import scipy.stats as stats\n",
        "\n",
        "# Import warnings\n",
        "import warnings\n",
        "\n",
        "# Import Feature Engine\n",
        "from feature_engine.outliers import Winsorizer\n",
        "from feature_engine.encoding import OrdinalEncoder\n",
        "from feature_engine.transformation import LogTransformer, PowerTransformer, BoxCoxTransformer, YeoJohnsonTransformer, ReciprocalTransformer\n",
        "\n",
        "# Enable inline plotting\n",
        "%matplotlib inline\n",
        "\n",
        "# Set theme style\n",
        "sns.set(style=\"darkgrid\")\n",
        "\n",
        "# Ignore warning\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "# Define feature engineering analysis function\n",
        "def FeatureEngineeringAnalysis(df, analysis_type=None):\n",
        "    \"\"\"\"\n",
        "    Used for quick feature engineering on numerical and categorical variables\n",
        "    to decide which transformation can better transform the distribution shape.\n",
        "    Once transformed, use a reporting tool, like ydata-profiling, to evaluate distributions.\n",
        "    \"\"\"\n",
        "    # Check for missing values\n",
        "    check_missing_values(df)\n",
        "    allowed_types = ['numerical', 'ordinal_encoder', 'outlier_winsorizer']\n",
        "    check_user_entry_on_analysis_type(analysis_type, allowed_types)\n",
        "    list_column_transformers = define_list_column_transformers(analysis_type)\n",
        "\n",
        "    # Loop through each variable and engineer the data according to the analysis type\n",
        "    df_feat_eng = pd.DataFrame([])\n",
        "    for column in df.columns:\n",
        "        # Create additional columns (column_method) to apply the methods\n",
        "        df_feat_eng = pd.concat([df_feat_eng, df[column]], axis=1)\n",
        "        for method in list_column_transformers:\n",
        "            df_feat_eng[f\"{column}_{method}\"] = df[column]\n",
        "\n",
        "        # Apply transformers to respective column_transformers\n",
        "        df_feat_eng, list_applied_transformers = apply_transformers(\n",
        "            analysis_type, df_feat_eng, column)\n",
        "\n",
        "        # For each variable, assess how the transformations perform\n",
        "        transformer_evaluation(\n",
        "            column, list_applied_transformers, analysis_type, df_feat_eng)\n",
        "\n",
        "    return df_feat_eng\n",
        "\n",
        "# Check analysis type\n",
        "def check_user_entry_on_analysis_type(analysis_type, allowed_types):\n",
        "    if analysis_type is None:\n",
        "        raise SystemExit(\n",
        "            f\"You should pass analysis_type parameter as one of the following options: {allowed_types}\")\n",
        "    if analysis_type not in allowed_types:\n",
        "        raise SystemExit(\n",
        "            f\"analysis_type argument should be one of these options: {allowed_types}\")\n",
        "\n",
        "# Check for missing values in the dataset\n",
        "def check_missing_values(df):\n",
        "    if df.isna().sum().sum() != 0:\n",
        "        raise SystemExit(\n",
        "            f\"There is a missing value in your dataset. Please handle that before getting into feature engineering.\")\n",
        "\n",
        "# Set suffix columns according to analysis_type\n",
        "def define_list_column_transformers(analysis_type):\n",
        "\n",
        "    # Define the list of column transformers based on the analysis type\n",
        "    if analysis_type == 'numerical':\n",
        "        list_column_transformers = [\n",
        "            \"log_e\", \"log_10\", \"reciprocal\", \"power\", \"box_cox\", \"yeo_johnson\"]\n",
        "    elif analysis_type == 'ordinal_encoder':\n",
        "        list_column_transformers = [\"ordinal_encoder\"]\n",
        "    elif analysis_type == 'outlier_winsorizer':\n",
        "        list_column_transformers = ['iqr']\n",
        "\n",
        "    return list_column_transformers\n",
        "\n",
        "# Apply transformations based on the analysis type.\n",
        "def apply_transformers(analysis_type, df_feat_eng, column):\n",
        "\n",
        "    # Convert categorical columns to 'object' type\n",
        "    for col in df_feat_eng.select_dtypes(include='category').columns:\n",
        "        df_feat_eng[col] = df_feat_eng[col].astype('object')\n",
        "    if analysis_type == 'numerical':\n",
        "        df_feat_eng, list_applied_transformers = FeatEngineering_Numerical(\n",
        "            df_feat_eng, column)\n",
        "    elif analysis_type == 'outlier_winsorizer':\n",
        "        df_feat_eng, list_applied_transformers = FeatEngineering_OutlierWinsorizer(\n",
        "            df_feat_eng, column)\n",
        "    elif analysis_type == 'ordinal_encoder':\n",
        "        df_feat_eng, list_applied_transformers = FeatEngineering_CategoricalEncoder(\n",
        "            df_feat_eng, column)\n",
        "\n",
        "    return df_feat_eng, list_applied_transformers\n",
        "\n",
        "# Evaluate transformations for each variable.\n",
        "def transformer_evaluation(column, list_applied_transformers, analysis_type, df_feat_eng):\n",
        "    print(f\"*** Variable Analyzed: {column}\")\n",
        "    print(f\"*** Applied transformation: {list_applied_transformers} \\n\")\n",
        "\n",
        "    # Display diagnostic plots\n",
        "    for col in [column] + list_applied_transformers:\n",
        "        if analysis_type != 'ordinal_encoder':\n",
        "            DiagnosticPlots_Numerical(df_feat_eng, col)\n",
        "\n",
        "        else:\n",
        "            if col == column:\n",
        "                DiagnosticPlots_Categories(df_feat_eng, col)\n",
        "            else:\n",
        "                DiagnosticPlots_Numerical(df_feat_eng, col)\n",
        "\n",
        "        print(\"\\n\")\n",
        "\n",
        "# Categorical plots diagnostics.\n",
        "def DiagnosticPlots_Categories(df_feat_eng, col):\n",
        "    if col not in df_feat_eng.columns:\n",
        "        raise ValueError(f\"Column {col} is not a valid column in the DataFrame.\")\n",
        "    \n",
        "    unique_categories = len(df_feat_eng[col].unique())\n",
        "    palette = sns.color_palette(\"Spectral\", n_colors=unique_categories)\n",
        "\n",
        "    # Countplot\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    sns.countplot(\n",
        "        data=df_feat_eng,\n",
        "        x=col,\n",
        "        palette=palette,\n",
        "        order=df_feat_eng[col].value_counts().index\n",
        "    )\n",
        "    plt.xticks(rotation=90)\n",
        "    plt.suptitle(f\"{col}\", fontsize=20, y=1.05)\n",
        "    plt.show()\n",
        "\n",
        "# Numerical plots diagnostics.\n",
        "def DiagnosticPlots_Numerical(df, variable):\n",
        "    \"\"\"Plot diagnostics for numerical variables.\"\"\"\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
        "\n",
        "    # Histogram\n",
        "    sns.histplot(data=df,\n",
        "                x=variable,\n",
        "                kde=True,\n",
        "                element=\"step\",\n",
        "                ax=axes[0],\n",
        "                color=sns.color_palette(\"Spectral\")[0]\n",
        "    )\n",
        "    axes[0].set_title('Histogram', fontsize=8)\n",
        "\n",
        "    # QQ-Plot\n",
        "    stats.probplot(\n",
        "        df[variable],\n",
        "        dist=\"norm\",\n",
        "        plot=axes[1],\n",
        "    )\n",
        "    if len(axes[1].get_lines()) >= 2:\n",
        "        axes[1].get_lines()[0].set_color(sns.color_palette(\"Spectral\")[4])\n",
        "        axes[1].get_lines()[1].set_color('black')\n",
        "    axes[1].set_title('QQ Plot', fontsize=9)\n",
        "\n",
        "    # Boxplot\n",
        "    sns.boxplot(\n",
        "        x=df[variable],\n",
        "        ax=axes[2], \n",
        "        color=sns.color_palette(\"Spectral\")[1]\n",
        "    )    \n",
        "    axes[2].set_title('Boxplot', fontsize=9)\n",
        "\n",
        "    # Save the figure and show the plot\n",
        "    fig.suptitle(f\"{variable}\", fontsize=20, y=1.05)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Apply ordinal encoding to categorical variable.\n",
        "def FeatEngineering_CategoricalEncoder(df_feat_eng, column):\n",
        "    list_methods_worked = []\n",
        "    try:\n",
        "        encoder = OrdinalEncoder(encoding_method='arbitrary', variables=[f\"{column}_ordinal_encoder\"])\n",
        "        df_feat_eng = encoder.fit_transform(df_feat_eng)\n",
        "        list_methods_worked.append(f\"{column}_ordinal_encoder\")\n",
        "\n",
        "    except Exception:\n",
        "        df_feat_eng.drop([f\"{column}_ordinal_encoder\"], axis=1, inplace=True)\n",
        "\n",
        "    return df_feat_eng, list_methods_worked\n",
        "\n",
        "# Apply Winsorization to handle outliers.\n",
        "def FeatEngineering_OutlierWinsorizer(df_feat_eng, column):\n",
        "    list_methods_worked = []\n",
        "\n",
        "    # Winsorizer iqr\n",
        "    try:\n",
        "        disc = Winsorizer(\n",
        "            capping_method='iqr', tail='both', fold=1.5, variables=[f\"{column}_iqr\"])\n",
        "        df_feat_eng = disc.fit_transform(df_feat_eng)\n",
        "        list_methods_worked.append(f\"{column}_iqr\")\n",
        "    except Exception:\n",
        "        df_feat_eng.drop([f\"{column}_iqr\"], axis=1, inplace=True)\n",
        "\n",
        "    return df_feat_eng, list_methods_worked\n",
        "\n",
        "# Apply numerical transformations.\n",
        "def FeatEngineering_Numerical(df_feat_eng, column):\n",
        "    list_methods_worked = []\n",
        "\n",
        "    # LogTransformer base e\n",
        "    try:\n",
        "        lt = LogTransformer(variables=[f\"{column}_log_e\"])\n",
        "        df_feat_eng = lt.fit_transform(df_feat_eng)\n",
        "        list_methods_worked.append(f\"{column}_log_e\")\n",
        "    except Exception:\n",
        "        df_feat_eng.drop([f\"{column}_log_e\"], axis=1, inplace=True)\n",
        "\n",
        "    # LogTransformer base 10\n",
        "    try:\n",
        "        lt = LogTransformer(variables=[f\"{column}_log_10\"], base='10')\n",
        "        df_feat_eng = lt.fit_transform(df_feat_eng)\n",
        "        list_methods_worked.append(f\"{column}_log_10\")\n",
        "    except Exception:\n",
        "        df_feat_eng.drop([f\"{column}_log_10\"], axis=1, inplace=True)\n",
        "\n",
        "    # ReciprocalTransformer\n",
        "    try:\n",
        "        rt = ReciprocalTransformer(variables=[f\"{column}_reciprocal\"])\n",
        "        df_feat_eng = rt.fit_transform(df_feat_eng)\n",
        "        list_methods_worked.append(f\"{column}_reciprocal\")\n",
        "    except Exception:\n",
        "        df_feat_eng.drop([f\"{column}_reciprocal\"], axis=1, inplace=True)\n",
        "\n",
        "    # PowerTransformer\n",
        "    try:\n",
        "        pt = PowerTransformer(variables=[f\"{column}_power\"])\n",
        "        df_feat_eng = pt.fit_transform(df_feat_eng)\n",
        "        list_methods_worked.append(f\"{column}_power\")\n",
        "    except Exception:\n",
        "        df_feat_eng.drop([f\"{column}_power\"], axis=1, inplace=True)\n",
        "\n",
        "    # BoxCoxTransformer\n",
        "    try:\n",
        "        bct = BoxCoxTransformer(variables=[f\"{column}_box_cox\"])\n",
        "        df_feat_eng = bct.fit_transform(df_feat_eng)\n",
        "        list_methods_worked.append(f\"{column}_box_cox\")\n",
        "    except Exception:\n",
        "        df_feat_eng.drop([f\"{column}_box_cox\"], axis=1, inplace=True)\n",
        "\n",
        "    # YeoJohnsonTransformer\n",
        "    try:\n",
        "        yjt = YeoJohnsonTransformer(variables=[f\"{column}_yeo_johnson\"])\n",
        "        df_feat_eng = yjt.fit_transform(df_feat_eng)\n",
        "        list_methods_worked.append(f\"{column}_yeo_johnson\")\n",
        "    except Exception:\n",
        "        df_feat_eng.drop([f\"{column}_yeo_johnson\"], axis=1, inplace=True)\n",
        "\n",
        "    return df_feat_eng, list_methods_worked"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2  Applying Feature Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next we will apply feature engineering techniques to prepare the `TrainSet` for modeling. This includes categorical encoding, numerical transformation and smart correlation selections.\n",
        "\n",
        "#### Data Types to Engineer:\n",
        "* Categorical \n",
        "* Numerical\n",
        "* Outlier\n",
        "* Smart Correlation Selection\n",
        "\n",
        "#### Steps:\n",
        "1. **Define variables**: Selecting categorical variables to be encoded.\n",
        "2. **Create new dataframe**: Create new dataframe with only the selected variables.\n",
        "3. **Feature Engineering and Visualization**: Feature engineer selected variables and visualize diagnostic plots.\n",
        "4. **Apply transformation**: Apply transformation on Train and Test Set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 2.2.1 Categorical Encoding\n",
        "Machine learning models require numerical inputs, so categorical variables are encoded using ordinal encoding. This method preserves the order of categories, which is important for variables like `GarageFinish` and `KitchenQual` that have a natural hierarchy.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the variables to be engineered and display them\n",
        "variables_engineering = ['BsmtExposure', 'BsmtFinType1', 'GarageFinish', 'KitchenQual']\n",
        "variables_engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create separate DataFrame with selected variables\n",
        "df_engineering = TrainSet[variables_engineering].copy()\n",
        "\n",
        "# Display the first few rows of the new dataframe\n",
        "df_engineering.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Encode categorical variables and display diagnostic plots\n",
        "df_engineering = FeatureEngineeringAnalysis(df=df_engineering, analysis_type='ordinal_encoder')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Perform categorical encoding on selected variables\n",
        "encoder = OrdinalEncoder(encoding_method='arbitrary', variables=variables_engineering)\n",
        "\n",
        "# Fit and transform the training set, transform the test set\n",
        "TrainSet = encoder.fit_transform(TrainSet)\n",
        "TestSet = encoder.transform(TestSet)\n",
        "\n",
        "# Display confirmation message\n",
        "print(\"*** Categorical encoding - ordinal transformation done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 2.2.2 Numerical Transformation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select the variables to be engineered and display them\n",
        "variables_engineering = [\n",
        "    '1stFlrSF', '2ndFlrSF', 'GrLivArea', 'LotArea',\n",
        "    'LotFrontage', 'GarageArea', 'MasVnrArea', 'OpenPorchSF', 'TotalBsmtSF'\n",
        "    ]\n",
        "variables_engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create separate DataFrame with selected variables\n",
        "df_engineering = TrainSet[variables_engineering].copy()\n",
        "\n",
        "# Display the first three rows of the new dataframe\n",
        "df_engineering.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Call `FeatureEngineeringAnalysis` function to apply numerical transformations to the selected variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Perform numerical transformation and display diagnostic plots\n",
        "df_engineering = FeatureEngineeringAnalysis(df=df_engineering, analysis_type='numerical')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### 2.2.1 Reduce skweness\n",
        "Log and power transformations are applied to reduce skewness and stabilize variance in numerical variables. This improves the model's ability to learn patterns from the data and ensures consistent scaling across features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create log and power transformers for selected variables\n",
        "lt = LogTransformer(variables=['GrLivArea', 'LotArea', 'LotFrontage'])\n",
        "pt = PowerTransformer(variables=['GarageArea', 'MasVnrArea', 'OpenPorchSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF'])\n",
        "\n",
        "# Create a list of transformers\n",
        "transformers = [lt, pt]\n",
        "for t in transformers:\n",
        "    # Fit and transform the training set, transform the test set\n",
        "    TrainSet = t.fit_transform(TrainSet)\n",
        "    TestSet = t.transform(TestSet)\n",
        "\n",
        "# Display confirmation message\n",
        "print(\"*** Numerical transformation done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 2.2.3 Winsorization\n",
        "Outliers can distort model performance by introducing noise. Winsorization caps extreme values to a specified range, reducing their impact while preserving the overall distribution of the data.\n",
        "\n",
        "##### Results\n",
        "The diagnostic plots show that Winsorization successfully reduced the impact of extreme values in variables like `GarageArea` and `MasVnrArea`, while maintaining the overall shape of the distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select the variables to be engineered and display them\n",
        "variables_engineering = [\n",
        "    'GarageArea', 'LotArea', 'LotFrontage', 'MasVnrArea',\n",
        "    'OpenPorchSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF'\n",
        "    ]\n",
        "variables_engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create separate DataFrame with the selected variables\n",
        "df_engineering = TrainSet[variables_engineering].copy()\n",
        "\n",
        "# Display the first three rows of the new dataframe\n",
        "df_engineering.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Apply Winsorization and display diagnostic plots\n",
        "df_engineering = FeatureEngineeringAnalysis(df=df_engineering, analysis_type='outlier_winsorizer')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Apply the Winsorization to the Train and Test datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Apply Winsorization to Train and Test sets to cap extreme values\n",
        "# This reduces the impact of outliers while preserving the overall distribution of the data.\n",
        "winsorizer = Winsorizer(capping_method='iqr', tail='both', fold=1.5, variables=variables_engineering)\n",
        "winsorizer = Winsorizer(capping_method='iqr', tail='both', fold=1.5, variables=variables_engineering)\n",
        "\n",
        "# Fit the Winsorizer on the training set\n",
        "TrainSet = winsorizer.fit_transform(TrainSet)\n",
        "TestSet = winsorizer.transform(TestSet)\n",
        "\n",
        "# Display confirmation message\n",
        "print(\"*** Winsorization done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 2.2.4 Smart Correlation Selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Highly correlated features can introduce multicollinearity, which negatively impacts regression models. Smart Correlation Selection identifies and removes redundant features based on `Spearman` correlation, retaining the most informative variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select variables to be engineered and display them\n",
        "variables_engineering = TrainSet.columns\n",
        "variables_engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create separate DataFrame with selected variables\n",
        "df_engineering = TrainSet[variables_engineering].copy()\n",
        "\n",
        "# Display the first three rows of the new dataframe\n",
        "df_engineering.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import for feature selection to handle multicollinearity\n",
        "from feature_engine.selection import SmartCorrelatedSelection\n",
        "\n",
        "# Initialize SmartCorrelatedSelection\n",
        "corr_sel = SmartCorrelatedSelection(variables=None, method=\"spearman\", threshold=0.8, selection_method=\"variance\")\n",
        "\n",
        "# Fit and transform the data\n",
        "df_engineering = corr_sel.fit_transform(df_engineering)\n",
        "\n",
        "# Display the results of correlated features and features to drop\n",
        "print(\"Correlated feature sets:\", corr_sel.correlated_feature_sets_)\n",
        "print(\"Features to drop:\", corr_sel.features_to_drop_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Update Train and Test sets with correlated features and dropped features\n",
        "TrainSet = TrainSet[df_engineering.columns]\n",
        "TestSet = TestSet[df_engineering.columns]\n",
        "\n",
        "# Display confirmation message\n",
        "print(\"*** Smart Correlation Selection done!\")\n",
        "print(f\"TrainSet shape: {TrainSet.shape}\")\n",
        "print(f\"TestSet shape: {TestSet.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltNetd085qHf"
      },
      "source": [
        "## Conclusion and Next Steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Conclusions\n",
        "We successfully engineered features to improve the predictive power of the dataset and prepared it for regression modeling. Key steps included handling missing values, transforming numerical variables, encoding categorical variables, and addressing multicollinearity. These steps ensure the dataset is clean, well-structured, and ready for the next phase of model training and evaluation.\n",
        "\n",
        "#### Key observations include:\n",
        "1. **Categorical Encoding**: Ordinal encoding was applied to variables like `GarageFinish` and `KitchenQual`, preserving the natural order of categories. This ensures that the encoded values reflect the hierarchy of the original categories, which is critical for regression models.\n",
        "2. **Numerical Transformations**: Log and power transformations effectively reduced skewness and stabilized variance in variables like `GrLivArea` and `LotArea`.\n",
        "3. **Outlier Handling**: Winsorization successfully capped extreme values in variables like `GarageArea` and `MasVnrArea`, reducing their impact on the model.\n",
        "4. **Feature Selection**: Smart Correlation Selection removed redundant features, ensuring the final dataset is free from multicollinearity.\n",
        "\n",
        "### Next Steps: Model Training and Evaluation\n",
        "1. **Model Training**: Train a regression model using the engineered features to predict house prices and experiment with different algorithms (e.g., linear regression, random forest) to identify the best-performing model.\n",
        "2. **Model Evaluation**: Evaluate the model's performance using metrics like RMSE, MAE, and R². Additionally, perform cross-validation to ensure the model generalizes well to unseen data.\n",
        "3. **Feature Importance Analysis**: Analyze the importance of the engineered features to understand their contribution to the model's predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Data Practitioner Jupyter Notebook.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
