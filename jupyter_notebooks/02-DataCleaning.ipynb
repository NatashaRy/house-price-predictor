{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# **02 - Data Exploration and Cleaning**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "* Clean the dataset to ensure it is ready for analysis and modeling.\n",
        "* Handle missing values, outliers, and inconsistent data to improve data quality.\n",
        "* Optimize data types for better performance and memory efficiency.\n",
        "* Ensure data integrity by addressing issues with file formats and missing values.\n",
        "* Save the cleaned dataset for further analysis and modeling.\n",
        "\n",
        "## Inputs\n",
        "\n",
        "* The raw dataset containing house price records: `inputs/datasets/raw/house_prices_records.csv`.\n",
        "\n",
        "## Outputs\n",
        "\n",
        "* An updated cleaned dataset saved as: `outputs/datasets/cleaned/house_prices_cleaned.parquet`.\n",
        "  - The `.parquet` format was chosen to preserve data types and handle missing values more reliably than `.csv`.\n",
        "\n",
        "## Additional Comments\n",
        "\n",
        "Due to issues with the `GarageFinish` column, where no missing values were present before saving but 235 missing values appeared when the saved file was loaded as a `.csv`, we have decided to use the `.parquet` format instead. \n",
        "\n",
        "The `.parquet` format preserves data types and handles missing values more reliably than `.csv`, ensuring the integrity of the cleaned dataset. This is crucial for maintaining data quality and consistency during analysis and modeling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWZXH9LwoQg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqP-UeN-z3i2"
      },
      "source": [
        "## Change Working Directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ensure the working directory is set to the project root for consistent file paths. This ensures that all file paths work correctly, regardless of where the notebook is executed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "wZfF_j-Bz3i4",
        "outputId": "66943449-1436-4c3d-85c7-b85f9f78349b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'d:\\\\Projects'"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "TwHsQRWjz3i9",
        "outputId": "86849db3-cd2f-4cc5-ebb8-2d0caafa1a2c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You set a new current directory\n"
          ]
        }
      ],
      "source": [
        "os.chdir(os.path.dirname(current_dir))\n",
        "print(\"You set a new current directory\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "vz3S-_kjz3jA",
        "outputId": "00b79ae4-75d0-4a96-d193-ac9ef9847ea2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'d:\\\\'"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mavJ8DibrcQ"
      },
      "source": [
        "## 1. Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'inputs/datasets/raw/house_prices_records.csv'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Create new DataFrame of raw dataset containing house prices\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m df_prices = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minputs/datasets/raw/house_prices_records.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Display the amount of rows/columns and first few rows of the dataset\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mShape of the dataset:\u001b[39m\u001b[33m\"\u001b[39m, df_prices.shape)\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\milestone-project-heritage-housing-issues\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:948\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m    935\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m    936\u001b[39m     dialect,\n\u001b[32m    937\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m    944\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m    945\u001b[39m )\n\u001b[32m    946\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m--> \u001b[39m\u001b[32m948\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\milestone-project-heritage-housing-issues\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:611\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    608\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    610\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m611\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    613\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    614\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\milestone-project-heritage-housing-issues\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1448\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1445\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1447\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1448\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\milestone-project-heritage-housing-issues\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1705\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1703\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1704\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1705\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1706\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1707\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1708\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1709\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1710\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1711\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1712\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1713\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1714\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1715\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1716\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\milestone-project-heritage-housing-issues\\.venv\\Lib\\site-packages\\pandas\\io\\common.py:863\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    858\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    859\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    860\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    861\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    862\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m863\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    864\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    865\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    866\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    867\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    868\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    869\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    870\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    871\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    872\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
            "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'inputs/datasets/raw/house_prices_records.csv'"
          ]
        }
      ],
      "source": [
        "# Import Pandas for data manipulation\n",
        "import pandas as pd\n",
        "\n",
        "# Create DataFrame of raw dataset containing house prices\n",
        "df_prices = pd.read_csv(\"inputs/datasets/raw/house_prices_records.csv\")\n",
        "\n",
        "# Display the amount of rows/columns and first few rows of the dataset\n",
        "print(\"Shape of the dataset:\", df_prices.shape)\n",
        "df_prices.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY3l0-AxO93d"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFQo3ycuO-v6"
      },
      "source": [
        "## 2. Data Exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1 Identify Missing Values\n",
        "In this step, we aim to understand the scope of missing data in the dataset. This is important because:\n",
        "- Missing values can introduce bias or errors in the analysis and modeling process.\n",
        "- Identifying the most affected columns helps prioritize cleaning efforts.\n",
        "\n",
        "#### Steps:\n",
        "1. Identify columns with missing values.\n",
        "2. Sort the columns in descending order to highlight the most affected variables.\n",
        "3. Create a summary table showing:\n",
        "   - The number of missing values.\n",
        "   - The percentage of missing values relative to the dataset size.\n",
        "   - The data type of each column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate the number of missing values for each column\n",
        "missing_values = df_prices.isnull().sum()\n",
        "\n",
        "# Filter columns with missing values and sort them in descending order\n",
        "# This helps prioritize variables with the most missing data\n",
        "missing_data = missing_values[missing_values > 0].sort_values(ascending=False)\n",
        "\n",
        "# Create a DataFrame to summarize missing data\n",
        "missing_data_df = pd.DataFrame({\n",
        "    \"Column\": missing_data.index,\n",
        "    \"Missing Values\": missing_data.values,\n",
        "    \"Percentage\": (missing_data.values / len(df_prices) * 100).round(2),\n",
        "    \"Datatype\": [df_prices[col].dtype for col in missing_data.index]\n",
        "})\n",
        "\n",
        "# Display the missing data summary\n",
        "missing_data_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 Generate Profiling Report\n",
        "Generate a detailed report for columns with missing values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <ins><a href=\"https://ydata.ai/register\">Upgrade to ydata-sdk</a></ins>\n",
              "                <p>\n",
              "                    Improve your data and profiling with ydata-sdk, featuring data quality scoring, redundancy detection, outlier identification, text validation, and synthetic data generation.\n",
              "                </p>\n",
              "            </div>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "NameError",
          "evalue": "name 'missing_data' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mydata_profiling\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ProfileReport\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Create a list of variables with missing data\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m vars_with_missing_data = \u001b[43mmissing_data\u001b[49m.index.tolist()\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Generate profile report\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m vars_with_missing_data:\n",
            "\u001b[31mNameError\u001b[39m: name 'missing_data' is not defined"
          ]
        }
      ],
      "source": [
        "# Import YData Profiling library for exploratory data analysis\n",
        "from ydata_profiling import ProfileReport\n",
        "\n",
        "# Create a list of variables with missing data\n",
        "vars_with_missing_data = missing_data.index.tolist()\n",
        "\n",
        "# Generate profile report and display in notebook\n",
        "if vars_with_missing_data:\n",
        "    profile = ProfileReport(df=df_prices[vars_with_missing_data], minimal=True)\n",
        "    profile.to_notebook_iframe()\n",
        "else:\n",
        "    # Display a message indicating no missing data\n",
        "    print(\"There are no variables with missing data\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Data Cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1 Handling Missing Values\n",
        "In this step, we address missing values to ensure the dataset is complete and ready for analysis. The following methods are used:\n",
        "- **Numeric columns**: Missing values are imputed with the median to avoid skewing the data.\n",
        "- **Categorical columns**: Logical values such as `'None'` or `'No'` are used to fill missing values, ensuring consistency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Impute missing values in numeric columns with the median\n",
        "df_prices['LotFrontage'].fillna(df_prices['LotFrontage'].median(), inplace=True)\n",
        "df_prices['BedroomAbvGr'].fillna(df_prices['BedroomAbvGr'].median(), inplace=True)\n",
        "df_prices['MasVnrArea'].fillna(df_prices['MasVnrArea'].median(), inplace=True)\n",
        "\n",
        "# Fill missing values for categorical variables\n",
        "df_prices['BsmtExposure'].fillna('No', inplace=True)\n",
        "df_prices['BsmtFinType1'].fillna('Unf', inplace=True)\n",
        "df_prices['GarageFinish'].fillna('None', inplace=True)\n",
        "\n",
        "# Fill missing values for numeric variables\n",
        "df_prices['2ndFlrSF'].fillna(0, inplace=True)\n",
        "df_prices['GarageYrBlt'].fillna(0, inplace=True)\n",
        "\n",
        "# Display the count of missing values after imputation\n",
        "print(df_prices.isnull().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 Dropping columns\n",
        "Dropping two columns since missing value is above 80%."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_prices.drop(columns=['EnclosedPorch', 'WoodDeckSF'], inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Split Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.1 Create New Directory and Save Cleaned Dataset\n",
        "Switch file format from `.csv` to `parquet` due to issues with missing values in `GarageFinish`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create output directory for cleaned data\n",
        "try:\n",
        "    os.makedirs(name='outputs/datasets/cleaned', exist_ok=True)\n",
        "except Exception as e:\n",
        "    print(e)\n",
        "\n",
        "# Save cleaned DataFrame in Parquet format\n",
        "df_prices.to_parquet(\"outputs/datasets/cleaned/house_prices_cleaned.parquet\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2 Split Data into Train and Test Set\n",
        "To prepare the dataset for modeling, we split it into training and testing sets:\n",
        "- **Training set**: Used to train the machine learning model, ensuring the model learns patterns in the data.\n",
        "- **Testing set**: Used to evaluate the model's performance on unseen data, providing an unbiased estimate of its accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import library\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load cleaned dataset\n",
        "df_cleaned = pd.read_parquet(\"outputs/datasets/cleaned/house_prices_cleaned.parquet\")\n",
        "\n",
        "# Split data into training and testing sets\n",
        "TrainSet, TestSet = train_test_split(\n",
        "    df_cleaned,\n",
        "    test_size=0.2,\n",
        "    random_state=42)\n",
        "\n",
        "# Display row and columns of training and test set\n",
        "print(f\"Train set shape: {TrainSet.shape}\")\n",
        "print(f\"Test set shape: {TestSet.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.3 Check Missing Values and Display Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train set\n",
        "print(f\"Missing values in TrainSet:\\n{TrainSet.isnull().sum()}\")\n",
        "\n",
        "# Test set\n",
        "print(f\"Missing values in TestSet:\\n{TestSet.isnull().sum()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Visualize Data Cleaning Effects"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `DataCleaningEffect` function is used to:\n",
        "1. **Verify the cleaning process**: Ensure that missing values have been handled correctly and that the data cleaning hasn't introduced any unexpected changes.\n",
        "2. **Compare original vs cleaned data**: See how the distributions of variables have changed after cleaning.\n",
        "3. **Document the process**: Provide clear visual evidence of the cleaning steps for transparency and reproducibility.\n",
        "\n",
        "### Results\n",
        "The visualizations below show the impact of data cleaning on selected variables. Key observations include:\n",
        "- **`LotFrontage`**: Missing values were imputed with the median, resulting in a smoother distribution.\n",
        "- **`GarageFinish`**: Missing values were filled with `'None'`, ensuring consistency in the categorical data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# Theme\n",
        "sns.set_theme(style=\"darkgrid\")\n",
        "\n",
        "# Function to visualize the effect of data cleaning\n",
        "def DataCleaningEffect(df_original, df_cleaned, variables_applied_with_method):\n",
        "    \"\"\"\n",
        "    Visualize the impact of data cleaning on selected variables.\n",
        "\n",
        "    Parameters:\n",
        "    - df_original (DataFrame): The original dataset before cleaning.\n",
        "    - df_cleaned (DataFrame): The cleaned dataset after processing.\n",
        "    - variables_applied_with_method (list): List of variables to analyze.\n",
        "\n",
        "    Returns:\n",
        "    - None: Displays bar charts for categorical variables and histograms for numeric variables.\n",
        "    \"\"\"\n",
        "    flag_count = 1\n",
        "\n",
        "    # Identify categorical variables\n",
        "    categorical_variables = df_original.select_dtypes(exclude=['number']).columns\n",
        "\n",
        "    # Loop through every variable in the list\n",
        "    for var in variables_applied_with_method:\n",
        "        print(f\"*** Distribution Effect Analysis After Data Cleaning Method on variable: {var}\")\n",
        "\n",
        "        if var in categorical_variables:\n",
        "            # For categorical variables, create a bar chart\n",
        "            df1 = pd.DataFrame({\"Type\": \"Original\", \"Value\": df_original[var]})\n",
        "            df2 = pd.DataFrame({\"Type\": \"Cleaned\", \"Value\": df_cleaned[var]})\n",
        "            dfAux = pd.concat([df1, df2], axis=0)\n",
        "            dfAux.reset_index(drop=True, inplace=True) \n",
        "\n",
        "            fig, axes = plt.subplots(figsize=(8, 4))\n",
        "            sns.countplot(\n",
        "                data=dfAux, \n",
        "                x=\"Value\", \n",
        "                hue=\"Type\", \n",
        "                palette=sns.color_palette(\"Spectral\", n_colors=2) \n",
        "                )\n",
        "            axes.set_title(f\"Distribution Plot {flag_count}: {var}\")\n",
        "            plt.xticks(rotation=90)\n",
        "            plt.legend()\n",
        "            plt.show()\n",
        "            print(f\"*** Bar plot for categorical variable: {var}\")\n",
        "\n",
        "        else:\n",
        "            # For numeric variables, create histograms\n",
        "            fig, axes = plt.subplots(figsize=(8, 4))\n",
        "            sns.histplot(\n",
        "                data=df_original, \n",
        "                x=var, \n",
        "                color=sns.color_palette(\"Spectral\")[4],  \n",
        "                label='Original', \n",
        "                kde=True, \n",
        "                element=\"step\", \n",
        "                ax=axes\n",
        "                )\n",
        "            sns.histplot(\n",
        "                data=df_cleaned, \n",
        "                x=var, \n",
        "                color=sns.color_palette(\"Spectral\")[5],  \n",
        "                label='Cleaned', \n",
        "                kde=True, \n",
        "                element=\"step\", \n",
        "                ax=axes\n",
        "                )\n",
        "            axes.set_title(f\"Distribution Plot {flag_count}: {var}\")\n",
        "            plt.legend()\n",
        "            plt.show()\n",
        "            print(f\"*** Histogram for numerical variable: {var}\")\n",
        "\n",
        "        plt.close(fig)\n",
        "        flag_count += 1\n",
        "\n",
        "\n",
        "# Variables to verify\n",
        "variables_to_verify = ['LotFrontage', 'BedroomAbvGr', 'GarageFinish', 'MasVnrArea']\n",
        "\n",
        "# Call function to verify cleaning process\n",
        "DataCleaningEffect(df_original=df_prices, \n",
        "                   df_cleaned=TrainSet, \n",
        "                   variables_applied_with_method=variables_to_verify)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltNetd085qHf"
      },
      "source": [
        "## Conclusion and Next Steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Conclusions\n",
        "The data cleaning process was successfully completed, ensuring the dataset is ready for further analysis and modeling. Key steps and outcomes include:\n",
        "\n",
        "1. **Handling Missing Values**:\n",
        "   - Missing values in numeric columns (e.g., `LotFrontage`, `BedroomAbvGr`, `MasVnrArea`) were imputed using the median to preserve data integrity.\n",
        "   - Missing values in categorical columns (e.g., `GarageFinish`, `BsmtExposure`) were filled with logical values such as `'None'` or `'No'`.\n",
        "\n",
        "2. **Dropping Irrelevant Columns**:\n",
        "   - Columns with a high percentage of missing values (`EnclosedPorch`, `WoodDeckSF`) were removed to streamline the dataset and improve model performance.\n",
        "\n",
        "3. **Optimizing Data Types**:\n",
        "   - Categorical columns were converted to the `category` data type, reducing memory usage and improving computational efficiency.\n",
        "\n",
        "4. **Splitting Data**:\n",
        "   - The cleaned dataset was split into training and testing sets (80/20 split) to prepare for modeling.\n",
        "   - Both sets were verified to ensure no missing values remain.\n",
        "\n",
        "5. **Verification**:\n",
        "   - Visualizations were used to compare the original and cleaned datasets, confirming that the cleaning process was effective and introduced no unexpected changes.\n",
        "\n",
        "### Next Steps: Correlation Study\n",
        "1. **Formulate Hypotheses**:\n",
        "   - Identify key attributes likely to influence the target variable (`SalePrice`), such as `OverallQual`, `GrLivArea`, and `GarageFinish`.\n",
        "   - Develop hypotheses to test the relationships between these attributes and `SalePrice`.\n",
        "\n",
        "2. **Analyze Relationships**:\n",
        "   - Perform a correlation study to quantify the strength of relationships between selected attributes and `SalePrice`.\n",
        "   - Use statistical methods to validate the significance of these relationships.\n",
        "\n",
        "3. **Visualize Insights**:\n",
        "   - Create visualizations (e.g., heatmaps, scatterplots, boxplots) to explore and present the relationships between attributes and `SalePrice`.\n",
        "   - Highlight key predictors that will be used in the predictive model.\n",
        "\n",
        "4. **Prepare for Feature Engineering**:\n",
        "   - Based on the insights from the correlation study, identify attributes that may require transformation or scaling for modeling.\n",
        "   - Plan feature engineering steps to enhance model performance.\n",
        "\n",
        "### Save Files\n",
        "The cleaned and split datasets were saved in the `outputs/datasets/cleaned/` directory for future use:\n",
        "- **Train Set**: `train_set.parquet`\n",
        "- **Test Set**: `test_set.parquet`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aKlnIozA4eQO",
        "outputId": "fd09bc1f-adb1-4511-f6ce-492a6af570c0"
      },
      "outputs": [],
      "source": [
        "# Save Train Set\n",
        "TrainSet.to_parquet(\"outputs/datasets/cleaned/train_set.parquet\", index=False)\n",
        "\n",
        "# Save Test Set\n",
        "TestSet.to_parquet(\"outputs/datasets/cleaned/test_set.parquet\", index=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Data Practitioner Jupyter Notebook.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
